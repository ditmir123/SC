{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg-NdAi98asc"
   },
   "source": [
    "## Importing Required Libraries (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cyKq0_N__Ry",
    "outputId": "74d79329-e740-4272-c7bf-c69bee9ab9e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vkubade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vkubade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vkubade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vkubade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XgXH8N3GAkKP"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UpZ3e686__oG"
   },
   "outputs": [],
   "source": [
    "text_op='A psychological portrait of a woman in distress that manages to feel packed with feeling despite its airy presentation—but more than that, this is an angry book, and the shimmer off the character\\'s rage is gripping all on its own'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVStEqNs8ma0"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGq7XckmAegC",
    "outputId": "d9903072-38c4-4f41-a210-8a3327f7a91f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"A psychological portrait of a woman in distress that manages to feel packed with feeling despite its airy presentation—but more than that, this is an angry book, and the shimmer off the character's rage is gripping all on its own\"]\n"
     ]
    }
   ],
   "source": [
    "tokens_sents = nltk.sent_tokenize(text_op)\n",
    "print(tokens_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Nf8B9QxAgdF",
    "outputId": "3f83da7c-f8a1-4b76-e19c-032fe46930c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'psychological', 'portrait', 'of', 'a', 'woman', 'in', 'distress', 'that', 'manages', 'to', 'feel', 'packed', 'with', 'feeling', 'despite', 'its', 'airy', 'presentation—but', 'more', 'than', 'that', ',', 'this', 'is', 'an', 'angry', 'book', ',', 'and', 'the', 'shimmer', 'off', 'the', 'character', \"'s\", 'rage', 'is', 'gripping', 'all', 'on', 'its', 'own']\n"
     ]
    }
   ],
   "source": [
    "tokens_words = nltk.word_tokenize(text_op)\n",
    "print(tokens_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hp9zFaMo8pLU"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53Aa42kJAiE-",
    "outputId": "a15384b3-f1d0-4e5f-b21f-9a73df42cb9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'psycholog', 'portrait', 'of', 'a', 'woman', 'in', 'distress', 'that', 'manag', 'to', 'feel', 'pack', 'with', 'feel', 'despit', 'it', 'airi', 'presentation—but', 'more', 'than', 'that', ',', 'thi', 'is', 'an', 'angri', 'book', ',', 'and', 'the', 'shimmer', 'off', 'the', 'charact', \"'s\", 'rage', 'is', 'grip', 'all', 'on', 'it', 'own']\n"
     ]
    }
   ],
   "source": [
    "stem=[]\n",
    "for i in tokens_words:\n",
    "  ps = PorterStemmer()\n",
    "  stem_word= ps.stem(i)\n",
    "  stem.append(stem_word)\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1Gdi31b8rdM"
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2sBTqH2UAmGT",
    "outputId": "3e9d3577-1095-423a-9506-98f4ade1d6af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a psycholog portrait of a woman in distress that manag to feel pack with feel despit it airi presentation—but more than that , thi is an angri book , and the shimmer off the charact 's rage is grip all on it own\n"
     ]
    }
   ],
   "source": [
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in stem])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KaGqCAslAsex",
    "outputId": "fc221531-3001-4495-a582-26961a233ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'psycholog', 'portrait', 'of', 'a', 'woman', 'in', 'distress', 'that', 'manag', 'to', 'feel', 'pack', 'with', 'feel', 'despit', 'it', 'airi', 'presentation—but', 'more', 'than', 'that', ',', 'thi', 'is', 'an', 'angri', 'book', ',', 'and', 'the', 'shimmer', 'off', 'the', 'charact', \"'s\", 'rage', 'is', 'grip', 'all', 'on', 'it', 'own']\n"
     ]
    }
   ],
   "source": [
    "leme=[]\n",
    "for i in stem:\n",
    "  lemetized_word=lemmatizer.lemmatize(i)\n",
    "  leme.append(lemetized_word)\n",
    "print(leme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j40kX6zF8tfU"
   },
   "source": [
    "## POS (Parts of Speech) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fInYlozsAuDM",
    "outputId": "0fec2472-0f64-4736-95c5-e739b110ab4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts of Speech:  [('a', 'DT'), ('psycholog', 'NN'), ('portrait', 'NN'), ('of', 'IN'), ('a', 'DT'), ('woman', 'NN'), ('in', 'IN'), ('distress', 'NN'), ('that', 'WDT'), ('manag', 'VBZ'), ('to', 'TO'), ('feel', 'VB'), ('pack', 'NN'), ('with', 'IN'), ('feel', 'NN'), ('despit', 'NN'), ('it', 'PRP'), ('airi', 'VBZ'), ('presentation—but', 'RB'), ('more', 'JJR'), ('than', 'IN'), ('that', 'DT'), (',', ','), ('thi', 'EX'), ('is', 'VBZ'), ('an', 'DT'), ('angri', 'JJ'), ('book', 'NN'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('shimmer', 'JJR'), ('off', 'IN'), ('the', 'DT'), ('charact', 'NN'), (\"'s\", 'POS'), ('rage', 'NN'), ('is', 'VBZ'), ('grip', 'VBN'), ('all', 'DT'), ('on', 'IN'), ('it', 'PRP'), ('own', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Parts of Speech: \",nltk.pos_tag(leme))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6wQqm_n8xfL"
   },
   "source": [
    "## Stopwords in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vc6kR19_Avvw",
    "outputId": "48e34359-f958-4b42-b9b9-dea87bf49760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "sw_nltk = stopwords.words('english')\n",
    "print(sw_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFeIxDZF84nD"
   },
   "source": [
    "## Words that aren't stopwords in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfq0Xu21Axj3",
    "outputId": "eae71fa6-4d6c-4073-9b42-b1413a5d7039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psychological portrait woman distress manages feel packed feeling despite airy presentation—but that, angry book, shimmer character's rage gripping\n"
     ]
    }
   ],
   "source": [
    "words = [word for word in text_op.split() if word.lower() not in sw_nltk]\n",
    "new_text = \" \".join(words)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwXblxTg_JK8"
   },
   "source": [
    "## Calculating TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "7OuqgbRuJqez"
   },
   "outputs": [],
   "source": [
    "text_op='A psychological portrait of a woman in distress that manages to feel packed with feeling despite its airy presentation—but more than that, this is an angry book, and the shimmer off the character\\'s rage is gripping all on its own'\n",
    "text_op = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "RRniWKPzAy8N"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "034BhlZZ_0wM",
    "outputId": "f01ec5b0-0786-4dba-e334-8c46513126ee"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m tr_idf_model  \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m corpus \u001b[38;5;241m=\u001b[39m text_op\n\u001b[1;32m----> 3\u001b[0m tf_idf_vector \u001b[38;5;241m=\u001b[39m tr_idf_model\u001b[38;5;241m.\u001b[39mfit_transform(corpus)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(corpus)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2133\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2134\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2135\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2136\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2137\u001b[0m )\n\u001b[1;32m-> 2138\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1295\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1297\u001b[0m         )\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "tr_idf_model  = TfidfVectorizer(stop_words=None)\n",
    "corpus = text_op\n",
    "tf_idf_vector = tr_idf_model.fit_transform(corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3EShqgfALQr",
    "outputId": "09f798db-cdb7-4dc6-8846-f3446a76479c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_idf_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(tf_idf_vector))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf_idf_vector\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf_idf_vector' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(tf_idf_vector))\n",
    "print(tf_idf_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9v6hHGdpAvDL",
    "outputId": "b97d7643-3f9f-4d17-ce4e-b050a1fcd930"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_idf_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tf_idf_array \u001b[38;5;241m=\u001b[39m tf_idf_vector\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf_idf_array)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf_idf_vector' is not defined"
     ]
    }
   ],
   "source": [
    "tf_idf_array = tf_idf_vector.toarray()\n",
    "print(tf_idf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mo46eQVvA_qr",
    "outputId": "0660f6eb-da09-4873-ba7f-36ec22106089"
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m words_set \u001b[38;5;241m=\u001b[39m tr_idf_model\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(words_set)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1485\u001b[0m, in \u001b[0;36mCountVectorizer.get_feature_names_out\u001b[1;34m(self, input_features)\u001b[0m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_feature_names_out\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m \n\u001b[0;32m   1475\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;124;03m        Transformed feature names.\u001b[39;00m\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1485\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m   1487\u001b[0m         [t \u001b[38;5;28;01mfor\u001b[39;00m t, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m))],\n\u001b[0;32m   1488\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m,\n\u001b[0;32m   1489\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:508\u001b[0m, in \u001b[0;36m_VectorizerMixin._check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_vocabulary()\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_:\n\u001b[1;32m--> 508\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary not fitted or provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "words_set = tr_idf_model.get_feature_names_out()\n",
    "print(words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "EdSgx4evBGGj",
    "outputId": "4a5dc12a-ff2e-4dd9-e88d-3333056cec5e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_idf_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_tf_idf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tf_idf_array, columns \u001b[38;5;241m=\u001b[39m words_set)\n\u001b[0;32m      4\u001b[0m df_tf_idf\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf_idf_array' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_tf_idf = pd.DataFrame(tf_idf_array, columns = words_set)\n",
    "\n",
    "df_tf_idf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "oZZ7aqtkChQr",
    "outputId": "a1923474-5271-49e2-b8bf-0d4df44b4b67"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_tf_idf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_tf_idf\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_tf_idf' is not defined"
     ]
    }
   ],
   "source": [
    "df_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "3hDbgTGDKfk0",
    "outputId": "cb927d32-9e2f-476c-8212-a76a8463aa32"
   },
   "outputs": [],
   "source": [
    "# Plot of TF-IDF for word \"AIRY\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "df_tf_idf['airy'].plot(kind='line', figsize=(8, 4), title='airy')\n",
    "plt.gca().spines[['top', 'right']].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvWVeNCWKQyS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
